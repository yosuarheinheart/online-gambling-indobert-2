# app.py ‚Äî Streamlit app with robust HF loader + full preprocessing for inputs (text & YouTube comments)
import os
import re
import html
import string
import time
import traceback
from typing import List, Optional

import streamlit as st
import pandas as pd
import torch
import requests
import matplotlib.pyplot as plt

from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
from huggingface_hub import list_repo_files, hf_hub_download

# --- optional text libraries (Sastrawi) with graceful fallback ---
try:
    from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
    from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
    _SASTRAWI_AVAILABLE = True
except Exception:
    _SASTRAWI_AVAILABLE = False

# ---------------- Config ----------------
DEFAULT_REPO = "yossss90/indobert-imbalance-2"  # ganti sesuai repo HF Anda
st.set_page_config(page_title="IndoBERT Classifier", layout="centered", initial_sidebar_state="expanded")

# ---------------- Preprocessing utilities (copied & adapted) ----------------
FULL_UNICODE_NORMALIZATION_MAP = {
    # Full-width
    'Ôº°':'A', 'Ôº¢':'B', 'Ôº£':'C', 'Ôº§':'D', 'Ôº•':'E', 'Ôº¶':'F', 'Ôºß':'G', 'Ôº®':'H', 'Ôº©':'I', 'Ôº™':'J', 'Ôº´':'K', 'Ôº¨':'L', 'Ôº≠':'M', 'ÔºÆ':'N', 'ÔºØ':'O', 'Ôº∞':'P', 'Ôº±':'Q', 'Ôº≤':'R', 'Ôº≥':'S', 'Ôº¥':'T', 'Ôºµ':'U', 'Ôº∂':'V', 'Ôº∑':'W', 'Ôº∏':'X', 'Ôºπ':'Y', 'Ôº∫':'Z',
    'ÔΩÅ':'a', 'ÔΩÇ':'b', 'ÔΩÉ':'c', 'ÔΩÑ':'d', 'ÔΩÖ':'e', 'ÔΩÜ':'f', 'ÔΩá':'g', 'ÔΩà':'h', 'ÔΩâ':'i', 'ÔΩä':'j', 'ÔΩã':'k', 'ÔΩå':'l', 'ÔΩç':'m', 'ÔΩé':'n', 'ÔΩè':'o', 'ÔΩê':'p', 'ÔΩë':'q', 'ÔΩí':'r', 'ÔΩì':'s', 'ÔΩî':'t', 'ÔΩï':'u', 'ÔΩñ':'v', 'ÔΩó':'w', 'ÔΩò':'x', 'ÔΩô':'y', 'ÔΩö':'z',
    'Ôºê':'0', 'Ôºë':'1', 'Ôºí':'2', 'Ôºì':'3', 'Ôºî':'4', 'Ôºï':'5', 'Ôºñ':'6', 'Ôºó':'7', 'Ôºò':'8', 'Ôºô':'9',

    # Double-Struck
    'ùî∏':'A', 'ùîπ':'B', '‚ÑÇ':'C', 'ùîª':'D', 'ùîº':'E', 'ùîΩ':'F', 'ùîæ':'G', '‚Ñç':'H', 'ùïÄ':'I', 'ùïÅ':'J', 'ùïÇ':'K', 'ùïÉ':'L', 'ùïÑ':'M', '‚Ñï':'N', 'ùïÜ':'O', '‚Ñô':'P', '‚Ñö':'Q', '‚Ñù':'R', 'ùïä':'S', 'ùïã':'T', 'ùïå':'U', 'ùïç':'V', 'ùïé':'W', 'ùïè':'X', 'ùïê':'Y', '‚Ñ§':'Z',
    'ùïí':'a', 'ùïì':'b', 'ùïî':'c', 'ùïï':'d', 'ùïñ':'e', 'ùïó':'f', 'ùïò':'g', 'ùïô':'h', 'ùïö':'i', 'ùïõ':'j', 'ùïú':'k', 'ùïù':'l', 'ùïû':'m', 'ùïü':'n', 'ùï†':'o', 'ùï°':'p', 'ùï¢':'q', 'ùï£':'r', 'ùï§':'s', 'ùï•':'t', 'ùï¶':'u', 'ùïß':'v', 'ùï®':'w', 'ùï©':'x', 'ùï™':'y', 'ùï´':'z',

    # Mathematical Bold
    'ùêÄ':'A', 'ùêÅ':'B', 'ùêÇ':'C', 'ùêÉ':'D', 'ùêÑ':'E', 'ùêÖ':'F', 'ùêÜ':'G', 'ùêá':'H', 'ùêà':'I', 'ùêâ':'J', 'ùêä':'K', 'ùêã':'L', 'ùêå':'M', 'ùêç':'N', 'ùêé':'O', 'ùêè':'P', 'ùêê':'Q', 'ùêë':'R', 'ùêí':'S', 'ùêì':'T', 'ùêî':'U', 'ùêï':'V', 'ùêñ':'W', 'ùêó':'X', 'ùêò':'Y', 'ùêô':'Z',
    'ùêö':'a', 'ùêõ':'b', 'ùêú':'c', 'ùêù':'d', 'ùêû':'e', 'ùêü':'f', 'ùê†':'g', 'ùê°':'h', 'ùê¢':'i', 'ùê£':'j', 'ùê§':'k', 'ùê•':'l', 'ùê¶':'m', 'ùêß':'n', 'ùê®':'o', 'ùê©':'p', 'ùê™':'q', 'ùê´':'r', 'ùê¨':'s', 'ùê≠':'t', 'ùêÆ':'u', 'ùêØ':'v', 'ùê∞':'w', 'ùê±':'x', 'ùê≤':'y', 'ùê≥':'z',
    'ùüé':'0', 'ùüè':'1', 'ùüê':'2', 'ùüë':'3', 'ùüí':'4', 'ùüì':'5', 'ùüî':'6', 'ùüï':'7', 'ùüñ':'8', 'ùüó':'9',

    # Sans-Serif Bold Italic
    'ùòº':'A', 'ùòΩ':'B', 'ùòæ':'C', 'ùòø':'D', 'ùôÄ':'E', 'ùôÅ':'F', 'ùôÇ':'G', 'ùôÉ':'H', 'ùôÑ':'I', 'ùôÖ':'J', 'ùôÜ':'K', 'ùôá':'L', 'ùôà':'M', 'ùôâ':'N', 'ùôä':'O', 'ùôã':'P', 'ùôå':'Q', 'ùôç':'R', 'ùôé':'S', 'ùôè':'T', 'ùôê':'U', 'ùôë':'V', 'ùôí':'W', 'ùôì':'X', 'ùôî':'Y', 'ùôï':'Z',
    'ùôñ':'a', 'ùôó':'b', 'ùôò':'c', 'ùôô':'d', 'ùôö':'e', 'ùôõ':'f', 'ùôú':'g', 'ùôù':'h', 'ùôû':'i', 'ùôü':'j', 'ùô†':'k', 'ùô°':'l', 'ùô¢':'m', 'ùô£':'n', 'ùô§':'o', 'ùô•':'p', 'ùô¶':'q', 'ùôß':'r', 'ùô®':'s', 'ùô©':'t', 'ùô™':'u', 'ùô´':'v', 'ùô¨':'w', 'ùô≠':'x', 'ùôÆ':'y', 'ùôØ':'z',

    # Sans-Serif Bold
    'ùóî':'A', 'ùóï':'B', 'ùóñ':'C', 'ùóó':'D', 'ùóò':'E', 'ùóô':'F', 'ùóö':'G', 'ùóõ':'H', 'ùóú':'I', 'ùóù':'J', 'ùóû':'K', 'ùóü':'L', 'ùó†':'M', 'ùó°':'N', 'ùó¢':'O', 'ùó£':'P', 'ùó§':'Q', 'ùó•':'R', 'ùó¶':'S', 'ùóß':'T', 'ùó®':'U', 'ùó©':'V', 'ùó™':'W', 'ùó´':'X', 'ùó¨':'Y', 'ùó≠':'Z',
    'ùóÆ':'a', 'ùóØ':'b', 'ùó∞':'c', 'ùó±':'d', 'ùó≤':'e', 'ùó≥':'f', 'ùó¥':'g', 'ùóµ':'h', 'ùó∂':'i', 'ùó∑':'j', 'ùó∏':'k', 'ùóπ':'l', 'ùó∫':'m', 'ùóª':'n', 'ùóº':'o', 'ùóΩ':'p', 'ùóæ':'q', 'ùóø':'r', 'ùòÄ':'s', 'ùòÅ':'t', 'ùòÇ':'u', 'ùòÉ':'v', 'ùòÑ':'w', 'ùòÖ':'x', 'ùòÜ':'y', 'ùòá':'z',
    'ùü¨':'0', 'ùü≠':'1', 'ùüÆ':'2', 'ùüØ':'3', 'ùü∞':'4', 'ùü±':'5', 'ùü≤':'6', 'ùü≥':'7', 'ùü¥':'8', 'ùüµ':'9',

    # Monospace (untuk ùôøùöõùöòùöãùöéùöùùüæùüªùüª)
    'ùô∞':'A', 'ùô±':'B', 'ùô≤':'C', 'ùô≥':'D', 'ùô¥':'E', 'ùôµ':'F', 'ùô∂':'G', 'ùô∑':'H', 'ùô∏':'I', 'ùôπ':'J', 'ùô∫':'K', 'ùôª':'L', 'ùôº':'M', 'ùôΩ':'N', 'ùôæ':'O', 'ùôø':'P', 'ùöÄ':'Q', 'ùöÅ':'R', 'ùöÇ':'S', 'ùöÉ':'T', 'ùöÑ':'U', 'ùöÖ':'V', 'ùöÜ':'W', 'ùöá':'X', 'ùöà':'Y', 'ùöâ':'Z',
    'ùöä':'a', 'ùöã':'b', 'ùöå':'c', 'ùöç':'d', 'ùöé':'e', 'ùöè':'f', 'ùöê':'g', 'ùöë':'h', 'ùöí':'i', 'ùöì':'j', 'ùöî':'k', 'ùöï':'l', 'ùöñ':'m', 'ùöó':'n', 'ùöò':'o', 'ùöô':'p', 'ùöö':'q', 'ùöõ':'r', 'ùöú':'s', 'ùöù':'t', 'ùöû':'u', 'ùöü':'v', 'ùö†':'w', 'ùö°':'x', 'ùö¢':'y', 'ùö£':'z',
    'ùü∂':'0', 'ùü∑':'1', 'ùü∏':'2', 'ùüπ':'3', 'ùü∫':'4', 'ùüª':'5', 'ùüº':'6', 'ùüΩ':'7', 'ùüæ':'8', 'ùüø':'9',

    # Fraktur / Gothic (untuk ùïªùñöùñëùñÜùñöùñúùñéùñìùñü)
    'ùï¨':'A', 'ùï≠':'B', 'ùïÆ':'C', 'ùïØ':'D', 'ùï∞':'E', 'ùï±':'F', 'ùï≤':'G', 'ùï≥':'H', 'ùï¥':'I', 'ùïµ':'J', 'ùï∂':'K', 'ùï∑':'L', 'ùï∏':'M', 'ùïπ':'N', 'ùï∫':'O', 'ùïª':'P', 'ùïº':'Q', 'ùïΩ':'R', 'ùïæ':'S', 'ùïø':'T', 'ùñÄ':'U', 'ùñÅ':'V', 'ùñÇ':'W', 'ùñÉ':'X', 'ùñÑ':'Y', 'ùñÖ':'Z',
    'ùñÜ':'a', 'ùñá':'b', 'ùñà':'c', 'ùñâ':'d', 'ùñä':'e', 'ùñã':'f', 'ùñå':'g', 'ùñç':'h', 'ùñé':'i', 'ùñè':'j', 'ùñê':'k', 'ùñë':'l', 'ùñí':'m', 'ùñì':'n', 'ùñî':'o', 'ùñï':'p', 'ùññ':'q', 'ùñó':'r', 'ùñò':'s', 'ùñô':'t', 'ùñö':'u', 'ùñõ':'v', 'ùñú':'w', 'ùñù':'x', 'ùñû':'y', 'ùñü':'z',

    # Enclosed Alphanumerics (khusus untuk üÑøüÖÑüÑªüÑ∞üÖÑüÖÜüÑ∏üÑΩ)
    '‚í∂':'A', '‚í∑':'B', '‚í∏':'C', '‚íπ':'D', '‚í∫':'E', '‚íª':'F', '‚íº':'G', '‚íΩ':'H', '‚íæ':'I', '‚íø':'J', '‚ìÄ':'K', '‚ìÅ':'L', '‚ìÇ':'M', '‚ìÉ':'N', '‚ìÑ':'O', '‚ìÖ':'P', '‚ìÜ':'Q', '‚ìá':'R', '‚ìà':'S', '‚ìâ':'T', '‚ìä':'U', '‚ìã':'V', '‚ìå':'W', '‚ìç':'X', '‚ìé':'Y', '‚ìè':'Z',
    '‚ìê':'a', '‚ìë':'b', '‚ìí':'c', '‚ìì':'d', '‚ìî':'e', '‚ìï':'f', '‚ìñ':'g', '‚ìó':'h', '‚ìò':'i', '‚ìô':'j', '‚ìö':'k', '‚ìõ':'l', '‚ìú':'m', '‚ìù':'n', '‚ìû':'o', '‚ìü':'p', '‚ì†':'q', '‚ì°':'r', '‚ì¢':'s', '‚ì£':'t', '‚ì§':'u', '‚ì•':'v', '‚ì¶':'w', '‚ìß':'x', '‚ì®':'y', '‚ì©':'z',
    'üÖ∞':'A', 'üÖ±':'B', 'üÖ≤':'C', 'üÖ≥':'D', 'üÖ¥':'E', 'üÖµ':'F', 'üÖ∂':'G', 'üÖ∑':'H', 'üÖ∏':'I', 'üÖπ':'J', 'üÖ∫':'K', 'üÖª':'L', 'üÖº':'M', 'üÖΩ':'N', 'üÖæ':'O', 'üÖø':'P', 'üÜÄ':'Q', 'üÜÅ':'R', 'üÜÇ':'S', 'üÜÉ':'T', 'üÜÑ':'U', 'üÜÖ':'V', 'üÜÜ':'W', 'üÜá':'X', 'üÜà':'Y', 'üÜâ':'Z',
    'üÑø':'P', 'üÑæ':'O', 'üÑΩ':'N', 'üÑº':'M', 'üÑª':'L', 'üÑ∫':'K', 'üÑπ':'J', 'üÑ∏':'I', 'üÑ∑':'H', 'üÑ∂':'G', 'üÑµ':'F', 'üÑ¥':'E', 'üÑ≥':'D', 'üÑ≤':'C', 'üÑ±':'B', 'üÑ∞':'A',
    'üÖÄ':'Q', 'üÖÅ':'R', 'üÖÇ':'S', 'üÖÉ':'T', 'üÖÑ':'U', 'üÖÖ':'V', 'üÖÜ':'W', 'üÖá':'X', 'üÖà':'Y', 'üÖâ':'Z',

    # Karakter spesifik lain
    '⁄õ': 'S', 'õçÉ': 'A', 'õçÖ': 'G', 'õçÑ': 'A', 'ùÖô': 'A', 'õå∑': 'R', 'õå∫': 'D',
    '·ë≠': 'P', '·ñá': 'R', '·ó∑': 'B',
}

MULTI_CHAR_NORMALIZATION_MAP = {
    '0Ô∏è‚É£': '0', '1Ô∏è‚É£': '1', '2Ô∏è‚É£': '2', '3Ô∏è‚É£': '3', '4Ô∏è‚É£': '4', '5Ô∏è‚É£': '5', '6Ô∏è‚É£': '6', '7Ô∏è‚É£': '7', '8Ô∏è‚É£': '8', '9Ô∏è‚É£': '9',
    '‚ù∂': '1', '‚ù∑': '2', '‚ù∏': '3', '‚ùπ': '4', '‚ù∫': '5', '‚ùª': '6', '‚ùº': '7', '‚ùΩ': '8', '‚ùæ': '9',
}

def normalize_and_clean_styles(text: str) -> str:
    # multi-char mapping
    for old, new in MULTI_CHAR_NORMALIZATION_MAP.items():
        text = text.replace(old, new)

    # strip combining diacritics / zero-width / variation selectors
    diacritic_stripper = re.compile(r"[\u0300-\u036f\u0483-\u0489\u200b-\u200f\u20d0-\u20ff\ufe0e\ufe0f]")
    text = diacritic_stripper.sub('', text)

    # map characters via translation table
    trans_table = str.maketrans(FULL_UNICODE_NORMALIZATION_MAP)
    text = text.translate(trans_table)
    return text

def clean_text_modified(text: str) -> str:
    text = str(text)

    # remove anchor tags content
    text = re.sub(r'<a[^>]*>.*?</a>', ' ', text, flags=re.IGNORECASE | re.DOTALL)
    # remove any html tag
    text = re.sub(r'<[^>]+>', ' ', text)
    # remove urls (including something.tld/...)
    url_pattern = re.compile(r'(?:https?://|www\.)\S+|[a-zA-Z0-9-]+\.[a-zA-Z]{2,}(?:\/\S*)?')
    text = url_pattern.sub(' ', text)

    # normalize fancy unicode characters
    text = normalize_and_clean_styles(text)

    # unescape HTML entities
    text = html.unescape(text)

    # remove punctuation except keep hyphen '-'
    punc_to_remove = string.punctuation.replace('-', '')
    pattern = r'[' + re.escape(punc_to_remove) + r']'
    text = re.sub(pattern, ' ', text)

    # collapse whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def safe_stemmer(text: str, stemmer) -> str:
    new_tokens = []
    for token in text.split():
        if token.isalpha():
            try:
                new_tokens.append(stemmer.stem(token))
            except Exception:
                new_tokens.append(token)
        else:
            new_tokens.append(token)
    return " ".join(new_tokens)

# create sastrawi objects if available
if _SASTRAWI_AVAILABLE:
    try:
        stop_factory = StopWordRemoverFactory()
        stop_remover = stop_factory.create_stop_word_remover()
        stem_factory = StemmerFactory()
        stemmer = stem_factory.create_stemmer()
    except Exception:
        stop_remover = None
        stemmer = None
else:
    stop_remover = None
    stemmer = None

def preprocess_text_full(text: str) -> str:
    # 1) clean & normalize unicode / html / urls / punctuation
    t = clean_text_modified(text)
    # 2) lowercase
    t = t.lower()
    # 3) stopword remove (if available)
    if stop_remover is not None:
        try:
            t = stop_remover.remove(t)
        except Exception:
            pass
    # 4) safe stem
    if stemmer is not None:
        try:
            t = safe_stemmer(t, stemmer)
        except Exception:
            pass
    # final whitespace collapse
    t = re.sub(r'\s+', ' ', t).strip()
    return t

# ---------------- Automatic processing for DataFrame 'all_data' if present ----------------
# (Added so behavior matches the standalone script you provided)
try:
    if 'all_data' in globals() and isinstance(all_data, pd.DataFrame) and 'text' in all_data.columns:
        print("Memulai proses cleaning dengan kamus normalisasi definitif...")
        print("Langkah 1: Cleaning, normalisasi font, dan penghapusan URL...")
        all_data['clean_text'] = all_data['text'].apply(clean_text_modified)
        print("Langkah 2: Mengubah ke huruf kecil...")
        all_data['clean_text'] = all_data['clean_text'].str.lower()
        print("Langkah 3: Menghapus stopwords...")
        # Ensure stop_remover exists (try to initialize if Sastrawi available but was not initialized)
        if stop_remover is None and _SASTRAWI_AVAILABLE:
            try:
                stop_factory = StopWordRemoverFactory()
                stop_remover = stop_factory.create_stop_word_remover()
            except Exception:
                stop_remover = None
        if stop_remover is not None:
            all_data['clean_text'] = all_data['clean_text'].apply(lambda x: stop_remover.remove(x))
        else:
            print("Warning: Sastrawi stop_remover tidak tersedia; melewati tahap penghapusan stopwords.")
        print("Langkah 4: Melakukan stemming...")
        if stemmer is None and _SASTRAWI_AVAILABLE:
            try:
                stem_factory = StemmerFactory()
                stemmer = stem_factory.create_stemmer()
            except Exception:
                stemmer = None
        if stemmer is not None:
            all_data['clean_text'] = all_data['clean_text'].apply(lambda x: safe_stemmer(x, stemmer))
        else:
            print("Warning: Sastrawi stemmer tidak tersedia; melewati tahap stemming.")
        print("\nProses cleaning selesai.")
        print("\n--- Contoh Hasil pada 'all_data' ---")
        try:
            print(all_data[['text', 'clean_text']].head())
        except Exception:
            print("Tidak dapat menampilkan contoh hasil (mungkin environment tidak mendukung print DataFrame).")
except Exception as e:
    print(f"Auto-processing all_data failed: {e}")

# ---------------- Robust HF loader (same as before) ----------------
def find_model_subfolders(repo_id: str, token: Optional[str] = None) -> List[str]:
    files = list_repo_files(repo_id, token=token)
    folders = set()
    for f in files:
        if f.endswith("config.json"):
            if "/" in f:
                folders.add(f.rsplit("/", 1)[0])
            else:
                folders.add("")  # root
    nonroot = [f for f in folders if f]
    return nonroot + ([""] if "" in folders else [])

def try_from_pretrained(repo_id: str, subfolder: Optional[str], device: int, token: Optional[str] = None):
    kwargs = {}
    if token:
        kwargs["use_auth_token"] = token
    tok = AutoTokenizer.from_pretrained(repo_id, subfolder=subfolder, local_files_only=False, **kwargs)
    model = AutoModelForSequenceClassification.from_pretrained(repo_id, subfolder=subfolder, local_files_only=False, **kwargs)
    pipe = pipeline("text-classification", model=model, tokenizer=tok, return_all_scores=True, device=device)
    return pipe

def download_and_load_local(repo_id: str, subfolder: Optional[str], token: Optional[str], device: int):
    cache_root = os.path.join("model_cache", repo_id.replace("/", "_"))
    if subfolder:
        cache_dir = os.path.join(cache_root, subfolder.replace("/", "_"))
    else:
        cache_dir = os.path.join(cache_root, "root")
    os.makedirs(cache_dir, exist_ok=True)

    files = list_repo_files(repo_id, token=token)

    candidates = [f for f in files if f.endswith(".safetensors") or f.endswith(".bin") or f.endswith(".pt")]
    model_file = None
    if subfolder:
        for c in candidates:
            if c.startswith(subfolder + "/"):
                model_file = c
                break
    else:
        for c in candidates:
            if "/" not in c:
                model_file = c
                break
    if model_file is None and candidates:
        model_file = candidates[0]

    def join(folder, name):
        return f"{folder}/{name}" if folder else name

    needed = set()
    if model_file:
        needed.add(model_file)
    needed.add(join(subfolder or "", "config.json"))
    for name in ["tokenizer.json", "tokenizer_config.json", "vocab.txt", "merges.txt", "tokenizer.model"]:
        path = join(subfolder or "", name)
        if path in files:
            needed.add(path)

    for f in files:
        if subfolder:
            if f.startswith(subfolder + "/") and (f.endswith(".json") or f.endswith(".txt") or f.endswith(".model")):
                needed.add(f)
        else:
            if "/" not in f and (f.endswith(".json") or f.endswith(".txt") or f.endswith(".model")):
                needed.add(f)

    for fn in sorted(needed):
        if not fn or fn not in files:
            continue
        try:
            hf_hub_download(repo_id=repo_id, filename=fn, local_dir=cache_dir, token=token, local_dir_use_symlinks=False)
        except Exception as e:
            print(f"Warning: failed to download {fn}: {e}")

    tok = AutoTokenizer.from_pretrained(cache_dir, local_files_only=True)
    model = AutoModelForSequenceClassification.from_pretrained(cache_dir, local_files_only=True)
    pipe = pipeline("text-classification", model=model, tokenizer=tok, return_all_scores=True, device=device)
    return pipe

@st.cache_resource
def load_pipeline_hf(repo_id: str, device_choice: str = "auto"):
    hf_token = None
    try:
        hf_token = st.secrets["HF_TOKEN"]
    except Exception:
        hf_token = os.environ.get("HUGGINGFACE_TOKEN") or os.environ.get("HF_TOKEN")

    if device_choice == "cpu":
        device = -1
    elif device_choice == "gpu":
        device = 0
    else:
        device = 0 if torch.cuda.is_available() else -1

    try:
        return try_from_pretrained(repo_id, subfolder=None, device=device, token=hf_token), device
    except Exception as e_root:
        root_err = e_root
        print("Direct failed:", e_root)

    try:
        candidates = find_model_subfolders(repo_id, token=hf_token)
    except Exception as e_list:
        candidates = []
        print("list_repo_files failed:", e_list)

    for folder in candidates:
        try:
            pipe = try_from_pretrained(repo_id, subfolder=folder if folder else None, device=device, token=hf_token)
            return pipe, device
        except Exception as e:
            print(f"from_pretrained with subfolder='{folder}' failed:", e)

    fallback_folder = candidates[0] if candidates else ""
    try:
        pipe = download_and_load_local(repo_id, fallback_folder if fallback_folder else None, token=hf_token, device=device)
        return pipe, device
    except Exception as e_dl:
        print("Download-and-load failed:", e_dl)
        raise RuntimeError(
            "Failed to load model from Hugging Face repo. "
            f"Root error: {root_err}\nDownload fallback error: {e_dl}"
        )

# ---------------- Utility helpers ----------------
def get_top_prediction(scores_list):
    best = max(scores_list, key=lambda x: x["score"])
    return best["label"], float(best["score"])

def normalize_label(lbl: str):
    clean_lbl = lbl
    if isinstance(lbl, str):
        if lbl.startswith("LABEL_"):
            clean_lbl = lbl.replace("LABEL_", "")
    
    try:
        lbl_int = int(clean_lbl)
    except ValueError:
        return str(lbl)
    
    mapping = {
        0: "Netral",
        1: "Toxic",
        2: "Judol"
    }
    
    return mapping.get(lbl_int, f"Unknown ({lbl})")

# ---------------- YouTube helpers ----------------
YOUTUBE_API_URL = "https://www.googleapis.com/youtube/v3/commentThreads"

def extract_video_id(url: str) -> Optional[str]:
    regexes = [
        r"v=([A-Za-z0-9_-]{11})",
        r"youtu\.be/([A-Za-z0-9_-]{11})",
        r"youtube\.com/embed/([A-Za-z0-9_-]{11})",
        r"youtube\.com/v/([A-Za-z0-9_-]{11})",
        r"youtube\.com/watch\?.*v=([A-Za-z0-9_-]{11})"
    ]
    for r in regexes:
        m = re.search(r, url)
        if m:
            return m.group(1)
    if re.fullmatch(r"[A-Za-z0-9_-]{11}", url.strip()):
        return url.strip()
    return None

@st.cache_data(ttl=60*60)
def fetch_youtube_comments(video_id: str, api_key: str, max_comments: int = 200) -> List[str]:
    comments = []
    params = {
        "part": "snippet",
        "videoId": video_id,
        "maxResults": 100,
        "textFormat": "plainText",
        "key": api_key,
    }
    nextPageToken = None
    while True:
        if nextPageToken:
            params["pageToken"] = nextPageToken
        resp = requests.get(YOUTUBE_API_URL, params=params, timeout=30)
        if resp.status_code != 200:
            raise RuntimeError(f"YouTube API error {resp.status_code}: {resp.text}")
        data = resp.json()
        items = data.get("items", [])
        for it in items:
            try:
                text = it["snippet"]["topLevelComment"]["snippet"]["textDisplay"]
                comments.append(text)
                if len(comments) >= max_comments:
                    return comments[:max_comments]
            except Exception:
                continue
        nextPageToken = data.get("nextPageToken")
        if not nextPageToken:
            break
        time.sleep(0.1)
    return comments

# ---------------- Sidebar UI ----------------
st.sidebar.header("Settings")
repo_input = st.sidebar.text_input("Model repo / folder", value=DEFAULT_REPO,
                                  help="Hugging Face repo (username/repo) or local path")
device_opt = st.sidebar.selectbox("Device", options=["auto", "cpu", "gpu"], index=0,
                                  help="auto -> use GPU if available")
show_raw = st.sidebar.checkbox("Show raw scores (for debugging)", value=False)
example_btn = st.sidebar.button("Use example text")

# ---------------- Main UI ----------------
st.title("üß™ IndoBERT ‚Äî Text Classification")
st.subheader("Input: single text or YouTube link (analyze comments)")

# Load model (cached)
with st.spinner("Memuat model (sekali saja)..."):
    try:
        nlp, device_used = load_pipeline_hf(repo_input, device_choice=device_opt)
    except Exception as e:
        tb = traceback.format_exc()
        st.error(f"Gagal memuat model dari `{repo_input}`:\n{e}")
        st.code(tb)
        st.stop()

if device_used == 0:
    st.success("Model dimuat ‚Äî GPU akan digunakan untuk inference.")
else:
    st.info("Model dimuat ‚Äî CPU digunakan untuk inference.")

# choose input mode
mode = st.radio("Pilih mode input:", ["Text single", "YouTube URL (comments)"])

if example_btn:
    default_text = "Produk ini sangat memuaskan. Pengiriman cepat dan kualitasnya bagus."
else:
    default_text = ""

# ---------------- Text single mode (with preprocessing) ----------------
if mode == "Text single":
    text = st.text_area("Masukkan teks untuk diklasifikasi", value=default_text, height=140)
    if st.button("Predict single"):
        if not text or not text.strip():
            st.warning("Input tidak boleh kosong.")
        else:
            with st.spinner("Melakukan preprocessing & inference..."):
                pre = preprocess_text_full(text)
                out = nlp(pre)
                scores = out[0]
                top_label, top_score = get_top_prediction(scores)
                display_label = normalize_label(top_label)

            st.markdown("### üîé Prediksi Akhir")
            st.write("**Original:**", text)
            st.write("**Preprocessed:**", pre)
            st.metric(label="Predicted class", value=f"{display_label}", delta=f"{top_score:.4f}")
            st.caption("Probabilitas di metric adalah probabilitas kelas terpilih.")

            df = pd.DataFrame([{ "label": normalize_label(x["label"]), "score": x["score"] } for x in scores])
            df = df.sort_values("score", ascending=False).reset_index(drop=True)
            st.markdown("#### Probabilitas per Kelas")
            st.bar_chart(df.set_index("label"))

            if show_raw:
                st.markdown("#### Raw scores")
                st.json(scores)

# ---------------- YouTube comments mode (with preprocessing) ----------------
else:
    youtube_url = st.text_input("Masukkan link YouTube (atau langsung video id):", value="")
    max_comments = st.slider("Jumlah komentar maksimal", min_value=10, max_value=1000, value=200, step=10)
    analyze_btn = st.button("Analyze comments")

    if analyze_btn:
        vid = extract_video_id(youtube_url)
        if not vid:
            st.error("Tidak dapat mengekstrak video id. Pastikan URL benar.")
        else:
            api_key = None
            try:
                api_key = st.secrets["YOUTUBE_API_KEY"]
            except Exception:
                api_key = os.environ.get("YOUTUBE_API_KEY")
            if not api_key:
                st.error("YouTube API key tidak ditemukan. Set `YOUTUBE_API_KEY` di Streamlit secrets atau env var.")
            else:
                with st.spinner("Mengambil komentar dari YouTube..."):
                    try:
                        comments = fetch_youtube_comments(vid, api_key, max_comments=max_comments)
                    except Exception as e:
                        st.error(f"Gagal mengambil komentar: {e}")
                        comments = []

                if not comments:
                    st.warning("Tidak ada komentar yang berhasil diambil (atau komentar dinonaktifkan).")
                else:
                    st.success(f"Terambil {len(comments)} komentar ‚Äî menjalankan preprocessing & inference...")
                    batch_size = 32
                    preds = []
                    confidences = []
                    texts = []
                    preprocessed_texts = []
                    progress_bar = st.progress(0)
                    total = len(comments)
                    for i in range(0, total, batch_size):
                        batch = comments[i:i+batch_size]
                        # preprocess batch first
                        pre_batch = [preprocess_text_full(c) for c in batch]
                        try:
                            outs = nlp(pre_batch)
                        except Exception:
                            # fallback single
                            outs = [nlp(pb)[0] for pb in pre_batch]
                        for out in outs:
                            scores = out
                            label, conf = get_top_prediction(scores)
                            preds.append(normalize_label(label))
                            confidences.append(conf)
                        texts.extend(batch)
                        preprocessed_texts.extend(pre_batch)
                        progress_bar.progress(min(1.0, (i+batch_size)/total))
                    progress_bar.empty()

                    df_res = pd.DataFrame({
                        "comment": texts,
                        "preprocessed": preprocessed_texts,
                        "predicted_label": preds,
                        "confidence": confidences
                    })

                    counts = df_res["predicted_label"].value_counts()

                    st.markdown("### üìä Distribusi Kelas Komentar")
                    fig, ax = plt.subplots()

                    ax.pie(counts.values, labels=chart_labels, autopct='%1.1f%%', startangle=90)
                    ax.axis('equal')
                    st.pyplot(fig)
                    st.markdown("### üîé Tabel Hasil")
                    st.dataframe(df_res.head(200))

                    csv = df_res.to_csv(index=False)
                    st.download_button("Download hasil (CSV)", csv, file_name=f"yt_comments_pred_{vid}.csv", mime="text/csv")

                    if show_raw:
                        st.markdown("#### All predictions")
                        st.write(df_res)

# Footer / notes
st.markdown("---")
st.caption("Developed with ‚ù§Ô∏è by Group 4 - Deep Learning - 2025")